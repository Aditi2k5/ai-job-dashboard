# -*- coding: utf-8 -*-
"""rss feed

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CqCJw0XKKf8qIrYSWDgvBLF0EEu_JO5l
"""

#use olama, and use prompt for to extract jobtitles, (run llm on this nb)

!pip install feedparser nltk vader-sentiment wordcloud matplotlib
# Install NLTK if not already installed
!pip install nltk

import nltk

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('vader_lexicon')

import os
from nltk.data import find

# Check if 'averaged_perceptron_tagger' exists
try:
    path = find('taggers/averaged_perceptron_tagger')
    print(f"'averaged_perceptron_tagger' found at: {path}")
except LookupError:
    print("'averaged_perceptron_tagger' not found!")

import feedparser
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from collections import Counter
import re
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from bs4 import BeautifulSoup
import requests

import nltk

# Clear existing paths and set default locations
nltk.data.path = ["/usr/share/nltk_data", "/usr/local/share/nltk_data"]

# Download the resource to a known location
nltk.download('averaged_perceptron_tagger', download_dir='/usr/share/nltk_data')

# Verify the resource exists
from nltk.data import find
print(find('taggers/averaged_perceptron_tagger'))  # Should print a valid path

RSS_URL = "https://techcrunch.com/category/artificial-intelligence/feed/"
#RSS_URL =  "https://aijobs.net/feed/"


def fetch_articles(url):
    """Fetches and parses RSS feed."""
    feed = feedparser.parse(url)
    return feed.entries

# 2. Data Processing & Analysis
def extract_metadata(article):
    """Extracts key metadata from an article."""
    metadata = {
        "title": article.get("title", ""),
        "summary": BeautifulSoup(article.get("summary", ""), "html.parser").get_text(), # Remove HTML tags
        "link": article.get("link", ""),
        "published": article.get("published", ""),
        # Add more fields as needed (author, categories, etc.)
    }
    extracted_links = article.get("links", [])  # Extract links directly from article if present
    return metadata

def extract_links(article):
    """Extracts links from an article."""
    # Example implementation to extract links
    links = []
    # Get metadata within the function
    metadata = extract_metadata(article) # Add this line to get metadata
    # Your logic to extract links goes here
    # For example, if links are stored under 'links' key in article dictionary:
    links.append(metadata["link"])
    return links

def analyze_sentiment(text):
    """Performs sentiment analysis using VADER."""
    analyzer = SentimentIntensityAnalyzer()
    scores = analyzer.polarity_scores(text)
    # Simplify to positive/negative/neutral
    if scores['compound'] >= 0.05:
        return "Positive"
    elif scores['compound'] <= -0.05:
        return "Negative"
    else:
        return "Neutral"

import nltk
from nltk import pos_tag
from nltk.tokenize import word_tokenize
from collections import Counter
from bs4 import BeautifulSoup  # Make sure to import BeautifulSoup

# Download necessary NLTK resources (if not done already)
# Use download_dir to specify where to save the data
nltk.download('punkt', download_dir='/usr/share/nltk_data')
nltk.download('averaged_perceptron_tagger_eng', download_dir='/usr/share/nltk_data')  # Specify resource
nltk.download('stopwords', download_dir='/usr/share/nltk_data')
nltk.download('vader_lexicon', download_dir='/usr/share/nltk_data')
try:
      nltk.data.find('tokenizers/punkt_tab/english/')
except LookupError:
      nltk.download('punkt_tab', download_dir='/usr/share/nltk_data')


# Set the NLTK data path to include the download directory
nltk.data.path = ['/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']




def extract_tags(text, top_n=5):
    """Extracts most frequent nouns from text."""
    # Remove HTML tags if any
    text = BeautifulSoup(text, "html.parser").get_text()

    # Tokenize words
    words = word_tokenize(text.lower())

    # Perform POS tagging
    tagged_words = pos_tag(words)

    # Filter nouns (NN, NNP, NNPS, NNS)
    nouns = [word for word, pos in tagged_words if pos in ['NN', 'NNP', 'NNPS', 'NNS']]

    # Remove stop words
    stop_words = set(nltk.corpus.stopwords.words('english'))
    nouns = [noun for noun in nouns if noun not in stop_words]

    # Count noun occurrences
    noun_counts = Counter(nouns)

    return [noun for noun, count in noun_counts.most_common(top_n)]

# 3. Key Features

def generate_wordcloud(texts):
    """Generates a word cloud from a list of texts."""
    text = ' '.join(texts)
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show() #This will display the word cloud.  For a dashboard, you'd save the image.

def display_recent_articles(articles, num_articles=8):
    """Displays a summary of recent articles."""
    print("\n--- Recent Articles ---")
    for article in articles[:num_articles]:
        metadata = extract_metadata(article)
        sentiment = analyze_sentiment(metadata["summary"])
        tags = extract_tags(metadata["summary"])
        print(f"Title: {metadata['title']}")
        print(f"Summary: {metadata['summary']}")
        print(f"Sentiment: {sentiment}")
        print(f"Tags: {', '.join(tags)}")
        print(f"Link: {metadata['link']}\n")

import requests
from bs4 import BeautifulSoup
import time

def fetch_full_article(link):
    """Fixed TechCrunch article content extractor"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
    }

    try:
        response = requests.get(link, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')

        # TechCrunch's NEW article content structure
        article_body = soup.find('div', class_='wp-block-group') or \
                     soup.find('div', class_='article-content') or \
                     soup.find('article')

        if article_body:
            # Remove unwanted elements
            for elem in article_body.find_all(['script', 'style', 'aside', 'footer', 'figure']):
                elem.decompose()

            return article_body.get_text(separator='\n', strip=True)

        return "ARTICLE_CONTENT_NOT_FOUND"

    except Exception as e:
        print(f"Error: {str(e)}")
        return ""

# Modified job trends function
def extract_job_trends(articles):
    """Working job market analyzer"""
    risk_keywords = {'automation', 'layoffs', 'replaces', 'redundant', 'outsource'}
    demand_keywords = {'hiring', 'growth', 'expand', 'demand', 'skills'}

    jobs_at_risk, jobs_in_demand = 0, 0

    for idx, article in enumerate(articles[:3]):  # Test with first 3 articles
        print(f"\nProcessing article {idx+1}")
        metadata = extract_metadata(article)
        full_text = fetch_full_article(metadata['link'])

        if not full_text:
            print(f"Skipped: Failed to fetch {metadata['link']}")
            continue

        # Keyword analysis (case-insensitive)
        text_lower = full_text.lower()
        risk_found = any(kw in text_lower for kw in risk_keywords)
        demand_found = any(kw in text_lower for kw in demand_keywords)

        if risk_found:
            jobs_at_risk += 1
            print(f"Found risk keywords in: {metadata['title']}")
        if demand_found:
            jobs_in_demand += 1
            print(f"Found demand keywords in: {metadata['title']}")

        time.sleep(1)  # Respectful delay

    # Visualization
    plt.figure(figsize=(8, 4))
    plt.bar(['At Risk', 'In Demand'], [jobs_at_risk, jobs_in_demand],
           color=['#ff6b6b', '#4ecdc4'])
    plt.title("AI Job Impact Analysis (Live TechCrunch Data)")
    plt.ylabel("Number of Articles")
    plt.show()

if __name__ == "__main__":


  # Wait 1 second before next request

    articles = fetch_articles(RSS_URL)
    for article in articles:
    # Fetch article content

      if articles:
        display_recent_articles(articles)

        # Collect article summaries for word cloud
        summaries = [extract_metadata(article)['summary'] for article in articles]

        #generate_wordcloud(summaries)
        #extract_job_trends(articles)
        #extract_job_trends(articles)  # Add this line


        #display_job_market_trends(JOB_MARKET_DATA)

        #display_ai_investment_map() #Placeholder
    else:
        print("Failed to fetch articles.")

positive_links = []
negative_links = []
neutral_links = []
def categorize_links(articles):
    """Categorizes links based on sentiment analysis."""


    for article in articles:
        metadata = extract_metadata(article)
        sentiment = analyze_sentiment(metadata["summary"])
        link = metadata["link"]

        if sentiment == "Positive":
            positive_links.append(link)
        elif sentiment == "Negative":
            negative_links.append(link)
        else:  # Neutral
            neutral_links.append(link)

    print("Positive Links:")
    print(f"Number of Positive Links: {len(positive_links)}")
    for link in positive_links:
      print(link)

    print("\nNegative Links:")
    print(f"Number of Negative Links: {len(negative_links)}")
    for link in negative_links:
      print(link)

    print("\nNeutral Links:")
    print(f"Number of Neutral Links: {len(neutral_links)}")
    for link in neutral_links:
      print(link)


if __name__ == "__main__":
    articles = fetch_articles(RSS_URL)
    if articles:
        categorize_links(articles)
    else:
        print("Failed to fetch articles.")


flinks=positive_links + negative_links + neutral_links
print(flinks)

def categorize_links(articles):
    """Categorizes links based on sentiment analysis and provides counts."""
    positive_links = []
    negative_links = []
    neutral_links = []

    for article in articles:
        metadata = extract_metadata(article)
        sentiment = analyze_sentiment(metadata["summary"])
        link = metadata["link"]

        if sentiment == "Positive":
            positive_links.append(link)
        elif sentiment == "Negative":
            negative_links.append(link)
        else:  # Neutral
            neutral_links.append(link)

    print("Positive Links:")
    print(f"Number of Positive Links: {len(positive_links)}")
    #for link in positive_links:
      #print(link) #If you want to print the links, uncomment this

    print("\nNegative Links:")
    print(f"Number of Negative Links: {len(negative_links)}")
    #for link in negative_links:
      #print(link) #If you want to print the links, uncomment this

    print("\nNeutral Links:")
    print(f"Number of Neutral Links: {len(neutral_links)}")
    #for link in neutral_links:
      #print(link) #If you want to print the links, uncomment this


if __name__ == "__main__":
    articles = fetch_articles(RSS_URL)
    if articles:
        categorize_links(articles)
    else:
        print("Failed to fetch articles.")

import matplotlib.pyplot as plt

def categorize_and_plot(articles):
    """Categorizes links and creates a bar chart comparison."""
    positive_count = 0
    negative_count = 0
    neutral_count = 0

    for article in articles:
        metadata = extract_metadata(article)
        sentiment = analyze_sentiment(metadata["summary"])

        if sentiment == "Positive":
            positive_count += 1
        elif sentiment == "Negative":
            negative_count += 1
        else:  # Neutral
            neutral_count += 1

    categories = ["AI Creates Opportunities", "AI Diminishes Opportunities", "Neutral"]
    counts = [positive_count, negative_count, neutral_count]
    colors = ['green', 'red', 'blue']  # Assign colors to categories

    plt.figure(figsize=(8, 6))
    plt.bar(categories, counts, color=colors)
    plt.xlabel("Category")
    plt.ylabel("Number of Articles")
    plt.title("Sentiment Analysis of AI-related Articles")
    plt.show()

if __name__ == "__main__":
    articles = fetch_articles(RSS_URL)
    if articles:
        categorize_and_plot(articles)
    else:
        print("Failed to fetch articles.")

def summarize_articles(articles):
    """Analyzes articles and prints summaries with keywords."""
    for article in articles:
        metadata = extract_metadata(article)
        sentiment = analyze_sentiment(metadata["summary"])
        tags = extract_tags(metadata["summary"])
        print(f"Title: {metadata['title']}")
        print(f"Summary: {metadata['summary'][:200]}...")  # Limit summary length
        print(f"Sentiment: {sentiment}")
        print(f"Keywords: {', '.join(tags)}")
        print("-" * 40)  # Separator between articles

if __name__ == "__main__":
    articles = fetch_articles(RSS_URL)
    if articles:
        summarize_articles(articles)
    else:
        print("Failed to fetch articles.")

def extract_job_trends(articles):
    """Analyzes articles to identify jobs at risk and in demand due to AI."""
    risk_keywords = {'automation', 'layoffs', 'replace', 'redundant', 'outsource', 'job losses', 'displaced'}
    demand_keywords = {'hiring', 'growth', 'expand', 'demand', 'skills', 'new roles', 'opportunities', 'specialists'}
    # Expanded keyword lists for more comprehensive analysis

    jobs_at_risk = []  # Store articles related to job losses
    jobs_in_demand = []  # Store articles related to new job opportunities

    for article in articles[:10]:  # Analyze the first 10 articles for demonstration
        metadata = extract_metadata(article)
        full_text = fetch_full_article(metadata['link'])

        if not full_text:
            print(f"Skipped: Could not fetch content for {metadata['link']}")
            continue

        text_lower = full_text.lower()

        # Check for risk keywords
        if any(keyword in text_lower for keyword in risk_keywords):
            jobs_at_risk.append({
                "title": metadata['title'],
                "link": metadata['link'],
                "summary": metadata['summary'][:200] + "..." # Shortened summary
            })

        # Check for demand keywords
        if any(keyword in text_lower for keyword in demand_keywords):
            jobs_in_demand.append({
                "title": metadata['title'],
                "link": metadata['link'],
                "summary": metadata['summary'][:200] + "..."
            })
        time.sleep(1)

    print("\nJobs at Risk:")
    if jobs_at_risk:
        for job in jobs_at_risk:
            print(f"- {job['title']}: {job['summary']} ({job['link']})")
    else:
      print("No articles found indicating jobs at risk.")

    print("\nJobs in Demand:")
    if jobs_in_demand:
        for job in jobs_in_demand:
            print(f"- {job['title']}: {job['summary']} ({job['link']})")
    else:
      print("No articles found indicating jobs in demand.")

#Example usage
if __name__ == "__main__":
    articles = fetch_articles(RSS_URL)
    if articles:
        extract_job_trends(articles)
    else:
        print("Failed to fetch articles.")



# Commented out IPython magic to ensure Python compatibility.
!pip install colab-xterm
# %load_ext colabxterm

# Commented out IPython magic to ensure Python compatibility.
# %xterm

#curl https://ollama.ai/install.sh | sh
#ollama serve &
# ollama pull mistral

!ollama list

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U langchain-ollama

from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM

template = """Question: {question}

Answer: Let's think step by step."""

prompt = ChatPromptTemplate.from_template(template)

model = OllamaLLM(model="mistral")

chain = prompt | model

chain.invoke({"question": "What to do if i notice my friends are being toxic to me, and excluding me from plans"})

from bs4 import BeautifulSoup
import requests
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM

# my prompt
template = """
You are an AI assistant helping analyze news articles.

Analyze the following article and provide:

1. Specific job titles mentioned.
2. Jobs at risk of being replaced by AI.
3. Jobs that have already been replaced or heavily automated.
4. New job roles created by AI.
5. Skills that will remain valuable despite AI automation.
6. Skills being automated or replaced by AI.
7. Industry sectors being affected by this automation.
8. Information about funding going into AI development in this industry.

If the article is irrelevant to jobs or AI, respond ONLY with: SKIP.

Article: {article_content}

Answer:"""

prompt = ChatPromptTemplate.from_template(template)
model = OllamaLLM(model="mistral")  # or use another local model like "llama2"
chain = prompt | model

# Function to extract article text from a URL
def extract_article_text(url):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        return ' '.join(p.get_text() for p in soup.find_all('p'))
    except Exception as e:
        print(f"âŒ Failed to extract: {url} | {e}")
        return None

# Function to process the list of articles
def process_articles(article_links):
    job_insights = []

    for link in article_links:
        print(f"\nðŸ” Processing: {link}")
        content = extract_article_text(link)

        if not content:
            print("â›” SKIPPED (no content)")
            continue

        result = chain.invoke({"article_content": content}).strip()

        if result.upper().startswith("SKIP"):
            print("âš ï¸ SKIPPED (irrelevant content)")
            continue

        job_insights.append({"url": link, "insights": result})
        print("âœ… Insights extracted!")

    return job_insights

"""*italicized text*"""

# ðŸ”½ Replace this with your own list of article URLs
#flinks="https://techcrunch.com/category/artificial-intelligence/feed/"

results = process_articles(flinks[:5])

# ðŸ“„ Print clean results
for r in results:
    print(f"\nðŸ”— URL: {r['url']}\nðŸ§  Insights:\n{r['insights']}")

# Commented out IPython magic to ensure Python compatibility.
# SOME IDEAS
'''5. Interactive Mind Map or Tree View (Conceptual Map)
Structure:

Central Node: "AI and Jobs"

Branches: By Sector (e.g., Gaming, Journalism, Tech, Govt, etc.)

Each branch opens up to specific articles â†’ then insight bubbles (job replaced, created, etc.)

Vibe: Visual, nonlinear, exploratory.
Best for: Presenting connections, seeing big-picture themes, creative storytelling.
'''


'''
4. The "Job Lifecycle" Tracker
Concept: Track how jobs evolve, disappear, or emerge
Layout:

Timeline or Sankey diagram

Jobs grouped by:

âœ… Safe

âš ï¸ At Risk

âŒ Replaced

ðŸŒ± Newly Created

Click on a job role = opens related articles, insights, and future predictions

Use Case: Awesome for visually exploring transitions and job evolution due to AI.
'''


'''
5. Interactive AI Labor Map (World/Sector Map)
Layout:

Map or abstract bubble graph

Each sector = bubble/region

Click â†’ see key metrics:

# % of roles disrupted

Sample job titles

AI tools affecting it

Example companies or use cases

Links to all sourced articles with insights

Use Case: Ideal for a visually appealing overview that can zoom into depth.
'''



from bs4 import BeautifulSoup
import requests
import json
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM

# Updated prompt with fixed JSON formatting
template = """
You are an AI assistant analyzing news articles for job-related information. Analyze the following article and provide a JSON response with these fields:

1. "jobs_at_risk": Jobs identified as at risk of being replaced by AI.
2. "jobs_replaced": Jobs that have already been replaced or heavily automated by AI.
3. "new_ai_jobs": New job roles created by AI.
4. "skills_remaining": Skills likely to remain valuable despite AI automation.
5. "skills_automated": Skills currently being automated or replaced by AI.
6. "affected_industry": The primary industry sectors affected according to the article.
7. "funding_data": Information about funding going into AI development in this industry.

If the article is irrelevant to jobs or AI, respond with: {"status": "SKIP"}

Format your response as valid JSON only, with no additional text or explanation.

Article: {article_content}

JSON Response:"""

prompt = ChatPromptTemplate.from_template(template)
model = OllamaLLM(model="mistral")  # or use another local model like "llama2"
chain = prompt | model

def extract_article_text(url):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        return ' '.join(p.get_text() for p in soup.find_all('p'))
    except Exception as e:
        print(f"âŒ Failed to extract: {url} | {e}")
        return None

def process_articles(article_links):
    job_insights = []

    for link in article_links:
        print(f"\nðŸ” Processing: {link}")
        content = extract_article_text(link)

        if not content:
            print("â›” SKIPPED (no content)")
            continue

        try:
            result = chain.invoke({"article_content": content}).strip()
            # Clean the response by removing markdown code blocks if present
            result = result.replace('```json', '').replace('```', '').strip()

            try:
                data = json.loads(result)
                if isinstance(data, dict) and data.get("status") == "SKIP":
                    print("âš ï¸ SKIPPED (irrelevant content)")
                    continue

                # Ensure all expected fields are present
                required_fields = [ "jobs_at_risk", "jobs_replaced", "new_ai_jobs"]
                if not all(field in data for field in required_fields):
                    print("âš ï¸ SKIPPED (incomplete response)")
                    continue

                job_insights.append({"url": link, "insights": data})
                print("âœ… Insights extracted!")
            except json.JSONDecodeError as e:
                print(f"âš ï¸ SKIPPED (invalid JSON response: {e})")
                print(f"Raw response: {result}")
                continue

        except Exception as e:
            print(f"âš ï¸ Error processing article: {e}")
            continue

    return job_insights

# Assuming flinks is your list of article URLs
results = process_articles(flinks[:3])

# Save results to a JSON file
with open('job_insights.json', 'w') as f:
    json.dump(results, f, indent=2)

# Print clean results
print("\nFinal Results:")
for r in results:
    print(f"\nðŸ”— URL: {r['url']}")
    print("ðŸ§  Insights:")
    print(json.dumps(r['insights'], indent=2))

!pip install firebase-admin

!pip install mysql

!pip install mysql.connector

# prompt:  help me create a connection for a record in mysql to create tables in the database where it takes the credentials needed for the connection from a .env file stored here

import os
import mysql.connector
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

def create_mysql_connection():
    """Establishes a connection to MySQL using credentials from .env."""
    try:
        mydb = mysql.connector.connect(
            host=os.getenv("MYSQL_HOST"),
            user=os.getenv("MYSQL_USER"),
            password=os.getenv("MYSQL_PASSWORD"),
            database=os.getenv("MYSQL_DATABASE")
        )
        return mydb
    except mysql.connector.Error as err:
        print(f"Error connecting to MySQL: {err}")
        return None

def create_table(mydb, table_name, columns):
  """Creates a table in the database."""
  mycursor = mydb.cursor()
  try:
      create_table_query = f"CREATE TABLE IF NOT EXISTS {table_name} ({columns})"
      mycursor.execute(create_table_query)
      mydb.commit()
      print(f"Table '{table_name}' created successfully.")

  except mysql.connector.Error as err:
      print(f"Error creating table: {err}")

# Example usage
mydb = create_mysql_connection()

if mydb:
  # Define table and column details
  table_name = "job_insights"
  columns = """
      id INT AUTO_INCREMENT PRIMARY KEY,
      url VARCHAR(255),
      jobs_at_risk TEXT,
      jobs_replaced TEXT,
      new_ai_jobs TEXT,
      skills_remaining TEXT,
      skills_automated TEXT,
      affected_industry TEXT,
      funding_data TEXT
  """
  create_table(mydb, table_name, columns)
  mydb.close()

# prompt: help me create a connection for a record in mysql to create tables in the database

import mysql.connector

mydb = mysql.connector.connect(
  host="cs2002.webhostbox.net",
  user="scholdie_fow",
  password="43earth67cornersheet21seven19also41",
  database="scholdie_fow"
)

mycursor = mydb.cursor()

# Example: Create a table named 'customers'
mycursor.execute("CREATE TABLE IF NOT EXISTS customers (name VARCHAR(255), address VARCHAR(255))")

# Example: Insert data into the table
sql = "INSERT INTO customers (name, address) VALUES (%s, %s)"
val = ("John", "Highway 21")
mycursor.execute(sql, val)
mydb.commit()

print(mycursor.rowcount, "record inserted.")